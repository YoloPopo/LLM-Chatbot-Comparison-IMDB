{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxkDLFPttBPB"
      },
      "source": [
        "# 1. Language Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiwohSt3tWij",
        "outputId": "93a6fec8-1bee-495a-b623-70f0bd763e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19BNYmDAtBPC",
        "outputId": "85c1806e-9694-4d77-caa4-850c689fb4c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Attempting to load dataset from: imdb.csv\n",
            "Dataset loaded successfully.\n",
            "Loaded 50000 reviews.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import string\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "nltk.download(['punkt', 'wordnet', 'stopwords', 'punkt_tab'])\n",
        "\n",
        "# Set device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Load dataset\n",
        "data_path = \"imdb.csv\"\n",
        "print(f\"Attempting to load dataset from: {data_path}\")\n",
        "try:\n",
        "    # Attempt to read with default utf-8 encoding first\n",
        "    try:\n",
        "        df = pd.read_csv(data_path)\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"UTF-8 decoding failed, trying latin-1...\")\n",
        "        # Common alternative encoding for datasets found online\n",
        "        df = pd.read_csv(data_path, encoding='latin-1')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    # Basic cleaning: remove HTML tags\n",
        "    def clean_html(text):\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "    df['review'] = df['review'].apply(clean_html)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {data_path} not found.\")\n",
        "    print(\"Please download the 'IMDB Dataset of 50K Movie Reviews' from Kaggle\")\n",
        "    print(\"and place the 'imdb.csv' file (or the correct CSV file name) into the './data/' directory.\")\n",
        "    # Create a dummy dataframe to allow the rest of the notebook to run without crashing immediately\n",
        "    # You will need to replace this with the actual data loading for the code to work.\n",
        "    df = pd.DataFrame({'review': [\n",
        "        \"This is a placeholder review. Please load the real dataset.\",\n",
        "        \"Another placeholder review. The models need real data to train.\"\n",
        "    ]})\n",
        "    print(\"Created dummy dataframe. Please load the actual data for meaningful results.\")\n",
        "\n",
        "# Display dataframe info if loaded\n",
        "if 'review' in df.columns:\n",
        "    print(f\"Loaded {len(df)} reviews.\")\n",
        "else:\n",
        "    print(\"Warning: 'review' column not found in the loaded data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_mjqZr3tBPF"
      },
      "source": [
        "**Goal:** This notebook aims to implement, train, and compare three different language models on the IMDB movie review dataset:\n",
        "1.  A statistical n-gram model (Trigram with Add-1 smoothing).\n",
        "2.  A neural language model (LSTM) trained from scratch.\n",
        "3.  A pre-trained transformer model (GPT-2 small) fine-tuned on the dataset.\n",
        "\n",
        "We will evaluate these models based on their perplexity on held-out sentences (both grammatically correct and incorrect) and their ability to generate coherent text continuations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6fIlHItBPF"
      },
      "source": [
        "# 2. Dataset Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOjk8zZ2tBPF"
      },
      "source": [
        "The dataset used is the \"IMDB Dataset of 50K Movie Reviews\", commonly sourced from Kaggle or Stanford's AI repository. It contains 50,000 movie reviews, originally intended for binary sentiment classification, but here used solely for language modeling. The reviews are text-based and vary significantly in length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZb8YPEUtBPF",
        "outputId": "1c757152-ef20-4202-8312-ca342df08fd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 50000\n",
            "Calculating average tokens for 50000 reviews (this may take a moment)...\n",
            "Average number of tokens per review: 261.77\n",
            "Token calculation took 110.25 seconds.\n",
            "\n",
            "Example Reviews:\n",
            "\n",
            "Review 1:\n",
            "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname...\n",
            "\n",
            "Review 2:\n",
            "A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A...\n",
            "\n",
            "Review 3:\n",
            "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.This was the most...\n"
          ]
        }
      ],
      "source": [
        "print(f\"Total number of reviews: {len(df)}\")\n",
        "\n",
        "# Calculate average tokens per review\n",
        "total_tokens = 0\n",
        "num_reviews_for_avg = len(df) # Use all reviews for average calculation\n",
        "print(f\"Calculating average tokens for {num_reviews_for_avg} reviews (this may take a moment)...\")\n",
        "start_time = time.time()\n",
        "for review in df['review'].iloc[:num_reviews_for_avg]: # Iterate directly over the series\n",
        "    try:\n",
        "        tokens = word_tokenize(review.lower()) # Tokenize after lowercasing\n",
        "        total_tokens += len(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping a review due to tokenization error: {e}\")\n",
        "        num_reviews_for_avg -= 1 # Adjust count if a review fails\n",
        "\n",
        "if num_reviews_for_avg > 0:\n",
        "    average_tokens = total_tokens / num_reviews_for_avg\n",
        "    print(f\"Average number of tokens per review: {average_tokens:.2f}\")\n",
        "else:\n",
        "    print(\"Could not calculate average tokens (no valid reviews found).\")\n",
        "end_time = time.time()\n",
        "print(f\"Token calculation took {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "# Show a few example reviews\n",
        "print(\"\\nExample Reviews:\")\n",
        "for i, review in enumerate(df['review'].head(3)):\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    print(review[:500] + (\"...\" if len(review) > 500 else \"\")) # Print first 500 chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BC7taABptBPG"
      },
      "source": [
        "# 3. Statistical n-gram Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR3abDV4tBPG"
      },
      "source": [
        "We implement a Trigram (n=3) language model. This choice represents a balance between capturing some context (more than bigram) and managing data sparsity (less sparse than 4-grams).\n",
        "\n",
        "To handle unseen n-grams during evaluation, we use Add-1 (Laplace) smoothing. This is a simple smoothing technique that adds a constant value (k=1) to all n-gram counts.\n",
        "\n",
        "**Hyperparameters:**\n",
        "*   n = 3 (Trigram)\n",
        "*   k = 1 (Add-1 smoothing factor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRFy2QXltBPG",
        "outputId": "030e0aa3-17eb-4ed7-bd87-ad5dacdadfab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing 10000 reviews for n-gram model...\n",
            "Building vocabulary and n-gram counts...\n",
            "Vocabulary size: 77746\n",
            "Total trigrams counted: 1689050\n",
            "\n",
            "Calculating N-gram Perplexity...\n",
            "N-gram Perplexity on correct sentences: 5568.18\n",
            "N-gram Perplexity on incorrect sentences: 50105.90\n"
          ]
        }
      ],
      "source": [
        "# --- N-gram Model Implementation ---\n",
        "\n",
        "def preprocess_ngram(text, n):\n",
        "    \"\"\"Lowercase, tokenize, add start/end symbols.\"\"\"\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    # Add n-1 start symbols and 1 end symbol\n",
        "    return ['<s>'] * (n - 1) + tokens + ['</s>']\n",
        "\n",
        "# Use a subset of data for building n-gram counts to manage memory/time on CPU\n",
        "ngram_train_size = 10000 # Adjust as needed based on system resources\n",
        "print(f\"Preprocessing {ngram_train_size} reviews for n-gram model...\")\n",
        "ngram_corpus = [preprocess_ngram(text, n=3) for text in df['review'].iloc[:ngram_train_size]]\n",
        "\n",
        "# Build Vocabulary and Counts\n",
        "print(\"Building vocabulary and n-gram counts...\")\n",
        "vocab = set()\n",
        "unigram_counts = Counter()\n",
        "bigram_counts = defaultdict(int)\n",
        "trigram_counts = defaultdict(int)\n",
        "\n",
        "for sentence in ngram_corpus:\n",
        "    # Unigrams (for vocab and potential backoff/smoothing needs)\n",
        "    unigram_counts.update(sentence)\n",
        "    vocab.update(sentence)\n",
        "\n",
        "    # Bigrams\n",
        "    for w1, w2 in ngrams(sentence, 2):\n",
        "        bigram_counts[(w1, w2)] += 1\n",
        "\n",
        "    # Trigrams\n",
        "    for w1, w2, w3 in ngrams(sentence, 3):\n",
        "        trigram_counts[(w1, w2, w3)] += 1\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Total trigrams counted: {len(trigram_counts)}\")\n",
        "\n",
        "# Add-1 Smoothed Probability Function\n",
        "def get_add1_trigram_prob(w1, w2, w3, trigram_counts, bigram_counts, vocab_size, k=1):\n",
        "    \"\"\"Calculate Add-1 smoothed trigram probability P(w3 | w1, w2).\"\"\"\n",
        "    # Count(w1, w2, w3)\n",
        "    tri_count = trigram_counts.get((w1, w2, w3), 0)\n",
        "\n",
        "    # Count(w1, w2)\n",
        "    # Need to calculate this sum explicitly if not stored separately\n",
        "    # For simplicity here, we assume bigram_counts stores the count for the pair (w1, w2)\n",
        "    # A more robust way would be to sum trigram_counts[(w1, w2, *)]\n",
        "    # Let's recalculate Count(w1, w2) from trigrams for accuracy with add-1\n",
        "    # This is inefficient but conceptually correct for the formula\n",
        "    # A better approach stores N(w1, w2) = sum_w3 N(w1, w2, w3)\n",
        "    # Let's approximate using the bigram counts we gathered earlier\n",
        "    bi_count = bigram_counts.get((w1, w2), 0)\n",
        "\n",
        "    # P(w3 | w1, w2) = (Count(w1, w2, w3) + k) / (Count(w1, w2) + k * V)\n",
        "    probability = (tri_count + k) / (bi_count + k * vocab_size)\n",
        "    return probability\n",
        "\n",
        "# Perplexity Function\n",
        "def calculate_ngram_perplexity(sentences, prob_func, n, trigram_counts, bigram_counts, vocab_size, k=1):\n",
        "    \"\"\"Calculates perplexity for a list of sentences using a given probability function.\"\"\"\n",
        "    total_log_prob = 0\n",
        "    total_tokens = 0\n",
        "    num_sentences = 0\n",
        "\n",
        "    for sentence_tokens in sentences:\n",
        "        num_sentences += 1\n",
        "        # We predict tokens starting from the first word, up to and including </s>\n",
        "        # The number of predictions is len(sentence_tokens) - (n - 1)\n",
        "        sentence_log_prob = 0\n",
        "        num_predicted_tokens = 0\n",
        "\n",
        "        # Iterate through trigrams in the sentence\n",
        "        for i in range(n - 1, len(sentence_tokens)):\n",
        "            w1 = sentence_tokens[i - 2]\n",
        "            w2 = sentence_tokens[i - 1]\n",
        "            w3 = sentence_tokens[i]\n",
        "\n",
        "            # Replace OOV words with a special token if needed, or handle in prob_func\n",
        "            # For simplicity, Add-1 handles OOV implicitly by giving them non-zero probability\n",
        "            # if the context (w1, w2) was seen.\n",
        "            # If context (w1, w2) was never seen, bi_count=0, prob = k / (k*V) = 1/V\n",
        "\n",
        "            prob = prob_func(w1, w2, w3, trigram_counts, bigram_counts, vocab_size, k)\n",
        "\n",
        "            if prob > 0:\n",
        "                sentence_log_prob += math.log2(prob)\n",
        "            else:\n",
        "                # Should not happen with Add-1 smoothing unless V=0\n",
        "                sentence_log_prob += -float('inf') # Assign very low probability\n",
        "\n",
        "            num_predicted_tokens += 1\n",
        "\n",
        "        if num_predicted_tokens > 0:\n",
        "            total_log_prob += sentence_log_prob\n",
        "            total_tokens += num_predicted_tokens\n",
        "        else:\n",
        "            print(f\"Warning: Sentence too short for trigram model: {sentence_tokens}\")\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf') # Avoid division by zero\n",
        "\n",
        "    # Perplexity = 2^(-1/N * sum(log2(P(wi|...))))\n",
        "    avg_log_prob = total_log_prob / total_tokens\n",
        "    perplexity = math.pow(2, -avg_log_prob)\n",
        "    return perplexity\n",
        "\n",
        "# Define Held-out Sentences\n",
        "correct_sentences_text = [\n",
        "    \"This movie was surprisingly good.\",\n",
        "    \"I did not like the ending at all.\",\n",
        "    \"The acting was superb and the plot was engaging.\",\n",
        "    \"It felt like a waste of time.\",\n",
        "    \"She delivered a truly remarkable performance.\"\n",
        "    # Add 5 more if desired\n",
        "]\n",
        "\n",
        "incorrect_sentences_text = [\n",
        "    \"Good surprisingly was movie this.\",\n",
        "    \"Ending the like all not did I at.\",\n",
        "    \"Engaging plot the superb was acting and was.\",\n",
        "    \"Time of waste a like felt it.\",\n",
        "    \"Performance remarkable truly a delivered she.\"\n",
        "    # Add 5 more if desired\n",
        "]\n",
        "\n",
        "# Preprocess test sentences\n",
        "ngram_test_correct = [preprocess_ngram(s, n=3) for s in correct_sentences_text]\n",
        "ngram_test_incorrect = [preprocess_ngram(s, n=3) for s in incorrect_sentences_text]\n",
        "\n",
        "# Calculate and Report Perplexity\n",
        "print(\"\\nCalculating N-gram Perplexity...\")\n",
        "perplexity_correct = calculate_ngram_perplexity(\n",
        "    sentences=ngram_test_correct,\n",
        "    prob_func=get_add1_trigram_prob,\n",
        "    n=3,\n",
        "    trigram_counts=trigram_counts,\n",
        "    bigram_counts=bigram_counts,\n",
        "    vocab_size=vocab_size,\n",
        "    k=1\n",
        ")\n",
        "perplexity_incorrect = calculate_ngram_perplexity(\n",
        "    sentences=ngram_test_incorrect,\n",
        "    prob_func=get_add1_trigram_prob,\n",
        "    n=3,\n",
        "    trigram_counts=trigram_counts,\n",
        "    bigram_counts=bigram_counts,\n",
        "    vocab_size=vocab_size,\n",
        "    k=1\n",
        ")\n",
        "\n",
        "print(f\"N-gram Perplexity on correct sentences: {perplexity_correct:.2f}\")\n",
        "print(f\"N-gram Perplexity on incorrect sentences: {perplexity_incorrect:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh87upUBtBPG"
      },
      "source": [
        "**N-gram Perplexity Discussion:**\n",
        "\n",
        "*(Expected Outcome)* We typically expect the perplexity on the grammatically correct sentences to be significantly lower than on the incorrect sentences. This is because the trigram sequences in the correct sentences (e.g., \"movie was surprisingly\", \"was surprisingly good\") are more likely to have occurred in the training data (or have higher smoothed probabilities) than the jumbled sequences in the incorrect sentences (e.g., \"good surprisingly was\", \"surprisingly was movie\").\n",
        "\n",
        "A lower perplexity indicates the model is less \"surprised\" by the sequence, meaning it assigns higher probabilities to the observed words given their context. The large difference between the two scores would suggest the n-gram model has captured some basic sequential structure of the English language present in the reviews.\n",
        "\n",
        "However, the absolute perplexity values might still be relatively high, especially with simple Add-1 smoothing and a limited training subset. Data sparsity remains a challenge for n-gram models; many valid trigrams might not have been seen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9eYrvCgtBPH"
      },
      "source": [
        "# 4. Neural Network Model from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQEplWyitBPH"
      },
      "source": [
        "We will build an LSTM-based neural language model from scratch using PyTorch.\n",
        "\n",
        "**Architecture:**\n",
        "*   **Vocabulary Size:** Top 20,000 most frequent tokens from the training subset.\n",
        "*   **Embedding Layer:** Maps token indices to dense vectors of size 128.\n",
        "*   **LSTM Layers:** 2 stacked LSTM layers with a hidden state size of 256.\n",
        "*   **Dropout:** Applied with a probability of 0.2 between LSTM layers (if num_layers > 1) and potentially after the LSTM output.\n",
        "*   **Linear Layer:** Maps LSTM output features to scores for each word in the vocabulary.\n",
        "\n",
        "**Hyperparameters:**\n",
        "*   **Learning Rate (lr):** 1e-3 (0.001)\n",
        "*   **Batch Size:** 64\n",
        "*   **Epochs:** 5 (Adjust based on training time and convergence)\n",
        "*   **Weight Decay:** 1e-5 (for regularization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWHni1n8tBPH",
        "outputId": "46388d6a-750d-4282-d758-dc360b304bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- LSTM Model Setup ---\n",
            "Processing 10000 reviews for LSTM vocabulary and training...\n",
            "Actual LSTM vocabulary size: 20000\n",
            "Created LSTM Dataset with 10000 sequences.\n",
            "Total trainable parameters (LSTM): 8,621,600\n",
            "\n",
            "Starting LSTM training for 5 epochs...\n",
            "Epoch [1/5], Step [50/157], Loss: 6.6312\n",
            "Epoch [1/5], Step [100/157], Loss: 6.5858\n",
            "Epoch [1/5], Step [150/157], Loss: 6.6032\n",
            "\n",
            "Epoch 1 finished.\n",
            "Average Training Loss: 6.8126\n",
            "Epoch Time: 1061.94 seconds\n",
            "\n",
            "Epoch [2/5], Step [50/157], Loss: 6.5525\n",
            "Epoch [2/5], Step [100/157], Loss: 6.5135\n",
            "Epoch [2/5], Step [150/157], Loss: 6.4555\n",
            "\n",
            "Epoch 2 finished.\n",
            "Average Training Loss: 6.5362\n",
            "Epoch Time: 1041.55 seconds\n",
            "\n",
            "Epoch [3/5], Step [50/157], Loss: 6.3792\n",
            "Epoch [3/5], Step [100/157], Loss: 6.4058\n",
            "Epoch [3/5], Step [150/157], Loss: 6.2890\n",
            "\n",
            "Epoch 3 finished.\n",
            "Average Training Loss: 6.4207\n",
            "Epoch Time: 1056.47 seconds\n",
            "\n",
            "Epoch [4/5], Step [50/157], Loss: 6.1999\n",
            "Epoch [4/5], Step [100/157], Loss: 6.1245\n",
            "Epoch [4/5], Step [150/157], Loss: 6.1266\n",
            "\n",
            "Epoch 4 finished.\n",
            "Average Training Loss: 6.1993\n",
            "Epoch Time: 1044.20 seconds\n",
            "\n",
            "Epoch [5/5], Step [50/157], Loss: 5.9664\n",
            "Epoch [5/5], Step [100/157], Loss: 6.0087\n",
            "Epoch [5/5], Step [150/157], Loss: 6.0089\n",
            "\n",
            "Epoch 5 finished.\n",
            "Average Training Loss: 6.0056\n",
            "Epoch Time: 1034.84 seconds\n",
            "\n",
            "Finished LSTM training.\n",
            "Total Training Time: 5239.00 seconds\n",
            "\n",
            "Calculating LSTM Perplexity...\n",
            "LSTM Perplexity on correct sentences: 134.98\n",
            "LSTM Perplexity on incorrect sentences: 544.18\n"
          ]
        }
      ],
      "source": [
        "# --- LSTM Model Implementation ---\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# --- 1. Preprocessing and Vocabulary ---\n",
        "print(\"\\n--- LSTM Model Setup ---\")\n",
        "LSTM_VOCAB_SIZE = 20000\n",
        "LSTM_TRAIN_SIZE = 10000 # Use a subset for faster training on CPU\n",
        "MAX_LEN = 100 # Fixed sequence length for simplicity\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.2\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 5 # Adjust as needed\n",
        "\n",
        "print(f\"Processing {LSTM_TRAIN_SIZE} reviews for LSTM vocabulary and training...\")\n",
        "\n",
        "# Tokenize and build vocabulary\n",
        "all_tokens = []\n",
        "for review in df['review'].iloc[:LSTM_TRAIN_SIZE]:\n",
        "    tokens = word_tokenize(review.lower())\n",
        "    all_tokens.extend(tokens)\n",
        "\n",
        "token_counts = Counter(all_tokens)\n",
        "vocab = [word for word, count in token_counts.most_common(LSTM_VOCAB_SIZE - 2)] # Reserve space for PAD, UNK\n",
        "vocab_set = set(vocab)\n",
        "\n",
        "# Add special tokens\n",
        "PAD_TOKEN = \"<pad>\"\n",
        "UNK_TOKEN = \"<unk>\"\n",
        "if PAD_TOKEN not in vocab_set:\n",
        "    vocab.append(PAD_TOKEN)\n",
        "if UNK_TOKEN not in vocab_set:\n",
        "    vocab.append(UNK_TOKEN)\n",
        "\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "actual_vocab_size = len(word_to_idx)\n",
        "PAD_IDX = word_to_idx[PAD_TOKEN]\n",
        "UNK_IDX = word_to_idx[UNK_TOKEN]\n",
        "\n",
        "print(f\"Actual LSTM vocabulary size: {actual_vocab_size}\")\n",
        "\n",
        "# --- 2. PyTorch Dataset and DataLoader ---\n",
        "class ImdbDataset(Dataset):\n",
        "    def __init__(self, texts, word_to_idx, max_len, unk_idx):\n",
        "        self.sequences = []\n",
        "        self.targets = []\n",
        "        self.max_len = max_len\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = word_tokenize(text.lower())\n",
        "            indexed = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
        "\n",
        "            # Create input sequences and targets (next word prediction)\n",
        "            # Truncate long sequences\n",
        "            if len(indexed) > max_len:\n",
        "                indexed = indexed[:max_len]\n",
        "\n",
        "            # We need at least 2 tokens to form a pair (input, target)\n",
        "            if len(indexed) >= 2:\n",
        "                self.sequences.append(torch.tensor(indexed[:-1], dtype=torch.long))\n",
        "                self.targets.append(torch.tensor(indexed[1:], dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.targets[idx]\n",
        "\n",
        "def collate_fn(batch, pad_idx):\n",
        "    \"\"\"Pad sequences within a batch.\"\"\"\n",
        "    sequences, targets = zip(*batch)\n",
        "    # Pad sequences\n",
        "    sequences_padded = pad_sequence(sequences, batch_first=True, padding_value=pad_idx)\n",
        "    # Pad targets - crucial: use pad_idx or a specific ignore_index for loss\n",
        "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=pad_idx)\n",
        "    return sequences_padded, targets_padded\n",
        "\n",
        "# Create dataset and dataloader instances\n",
        "lstm_dataset = ImdbDataset(df['review'].iloc[:LSTM_TRAIN_SIZE].tolist(), word_to_idx, MAX_LEN, UNK_IDX)\n",
        "# Use functools.partial to pass pad_idx to collate_fn\n",
        "from functools import partial\n",
        "lstm_dataloader = DataLoader(lstm_dataset, batch_size=BATCH_SIZE,\n",
        "                           shuffle=True, collate_fn=partial(collate_fn, pad_idx=PAD_IDX))\n",
        "\n",
        "print(f\"Created LSTM Dataset with {len(lstm_dataset)} sequences.\")\n",
        "\n",
        "# --- 3. LSTM Model Definition ---\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, pad_idx):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text shape: [batch_size, seq_len]\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded shape: [batch_size, seq_len, embedding_dim]\n",
        "\n",
        "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
        "        # lstm_out shape: [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "        # Apply dropout to LSTM output before linear layer\n",
        "        dropped_out = self.dropout(lstm_out)\n",
        "\n",
        "        predictions = self.fc(dropped_out)\n",
        "        # predictions shape: [batch_size, seq_len, vocab_size]\n",
        "        return predictions\n",
        "\n",
        "# Instantiate the model\n",
        "lstm_model = LSTMModel(actual_vocab_size, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, PAD_IDX)\n",
        "lstm_model.to(device) # Move model to CPU\n",
        "\n",
        "# Compute and print total number of trainable parameters\n",
        "total_params_lstm = sum(p.numel() for p in lstm_model.parameters() if p.requires_grad)\n",
        "print(f'Total trainable parameters (LSTM): {total_params_lstm:,}')\n",
        "\n",
        "# --- 4. Training Loop ---\n",
        "optimizer = optim.AdamW(lstm_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "# Use CrossEntropyLoss - it combines LogSoftmax and NLLLoss\n",
        "# Important: set ignore_index to PAD_IDX so padding tokens don't contribute to loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "print(f\"\\nStarting LSTM training for {EPOCHS} epochs...\")\n",
        "lstm_training_start_time = time.time()\n",
        "\n",
        "lstm_model.train() # Set model to training mode\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for i, (sequences, targets) in enumerate(lstm_dataloader):\n",
        "        sequences, targets = sequences.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get predictions (logits)\n",
        "        # Shape: [batch_size, seq_len, vocab_size]\n",
        "        predictions = lstm_model(sequences)\n",
        "\n",
        "        # Reshape for CrossEntropyLoss\n",
        "        # Predictions need to be [batch_size * seq_len, vocab_size]\n",
        "        # Targets need to be [batch_size * seq_len]\n",
        "        predictions_flat = predictions.view(-1, actual_vocab_size)\n",
        "        targets_flat = targets.view(-1)\n",
        "\n",
        "        loss = criterion(predictions_flat, targets_flat)\n",
        "\n",
        "        loss.backward()\n",
        "        # Optional: Gradient clipping\n",
        "        # torch.nn.utils.clip_grad_norm_(lstm_model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Print progress periodically\n",
        "        if (i + 1) % 50 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{EPOCHS}], Step [{i+1}/{len(lstm_dataloader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    avg_epoch_loss = epoch_loss / len(lstm_dataloader)\n",
        "    print(f'\\nEpoch {epoch+1} finished.')\n",
        "    print(f'Average Training Loss: {avg_epoch_loss:.4f}')\n",
        "    print(f'Epoch Time: {epoch_end_time - epoch_start_time:.2f} seconds\\n')\n",
        "\n",
        "lstm_training_end_time = time.time()\n",
        "lstm_total_training_time = lstm_training_end_time - lstm_training_start_time\n",
        "print(f\"Finished LSTM training.\")\n",
        "print(f\"Total Training Time: {lstm_total_training_time:.2f} seconds\")\n",
        "\n",
        "# --- 5. LSTM Perplexity Calculation ---\n",
        "\n",
        "def preprocess_lstm_sentence(sentence_text, word_to_idx, unk_idx, device):\n",
        "    \"\"\"Tokenize and index a sentence for the LSTM model.\"\"\"\n",
        "    tokens = word_tokenize(sentence_text.lower())\n",
        "    indexed = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
        "    # Need input sequence (all but last) and target sequence (all but first)\n",
        "    if len(indexed) < 2:\n",
        "        return None, None\n",
        "    input_seq = torch.tensor([indexed[:-1]], dtype=torch.long).to(device) # Add batch dim\n",
        "    target_seq = torch.tensor([indexed[1:]], dtype=torch.long).to(device) # Add batch dim\n",
        "    return input_seq, target_seq\n",
        "\n",
        "def calculate_lstm_perplexity(model, sentences_text, word_to_idx, unk_idx, pad_idx, device):\n",
        "    \"\"\"Calculate perplexity for sentences using the trained LSTM model.\"\"\"\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, reduction='sum') # Sum losses, we average manually\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in sentences_text:\n",
        "            input_seq, target_seq = preprocess_lstm_sentence(text, word_to_idx, unk_idx, device)\n",
        "\n",
        "            if input_seq is None or target_seq is None:\n",
        "                print(f\"Skipping short sentence: {text}\")\n",
        "                continue\n",
        "\n",
        "            # Get model predictions (logits)\n",
        "            # Shape: [1, seq_len, vocab_size]\n",
        "            predictions = model(input_seq)\n",
        "\n",
        "            # Reshape for CrossEntropyLoss\n",
        "            # Predictions: [seq_len, vocab_size]\n",
        "            # Targets: [seq_len]\n",
        "            predictions_flat = predictions.squeeze(0) # Remove batch dim\n",
        "            targets_flat = target_seq.squeeze(0)     # Remove batch dim\n",
        "\n",
        "            loss = criterion(predictions_flat, targets_flat)\n",
        "\n",
        "            # Accumulate loss and count non-padding tokens in the target\n",
        "            total_loss += loss.item()\n",
        "            num_tokens = (targets_flat != pad_idx).sum().item()\n",
        "            total_tokens += num_tokens\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    # Average cross-entropy loss\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    # Perplexity is exp(average cross-entropy loss)\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return perplexity\n",
        "\n",
        "# Calculate and Report Perplexity\n",
        "print(\"\\nCalculating LSTM Perplexity...\")\n",
        "\n",
        "# Use the same sentences as the n-gram model\n",
        "lstm_perplexity_correct = calculate_lstm_perplexity(\n",
        "    lstm_model, correct_sentences_text, word_to_idx, UNK_IDX, PAD_IDX, device\n",
        ")\n",
        "lstm_perplexity_incorrect = calculate_lstm_perplexity(\n",
        "    lstm_model, incorrect_sentences_text, word_to_idx, UNK_IDX, PAD_IDX, device\n",
        ")\n",
        "\n",
        "print(f\"LSTM Perplexity on correct sentences: {lstm_perplexity_correct:.2f}\")\n",
        "print(f\"LSTM Perplexity on incorrect sentences: {lstm_perplexity_incorrect:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlbVxjOPtBPI"
      },
      "source": [
        "# 5. Fine-tuning a Pre-trained Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUCN_zGptBPI"
      },
      "source": [
        "We will fine-tune a pre-trained Generative Pre-trained Transformer 2 (GPT-2) model, specifically the smallest version (\"gpt2\"), which has 117 million parameters.\n",
        "\n",
        "**Model:** GPT-2 is a large transformer-based language model developed by OpenAI. It uses a decoder-only transformer architecture.\n",
        "\n",
        "**Pre-training:** It was pre-trained on a massive dataset (WebText) with the objective of predicting the next word in a sequence. This allows it to learn grammar, facts, and reasoning abilities, which can be adapted to specific downstream tasks like text generation on movie reviews through fine-tuning.\n",
        "\n",
        "**Fine-tuning:** We will use the Hugging Face `transformers` library to load the pre-trained model and tokenizer, and the `Trainer` API to fine-tune the model on the IMDB dataset. We will use a smaller batch size (8) due to potential memory constraints on CPU and train for only 1 or 2 epochs, as fine-tuning typically requires less training than training from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d3dad7881bc641feac8e765a88db8d7c",
            "f984c087688640bab1e60b748c05e9fb",
            "81f98b26c4204b1594ff5f6166e5cfce",
            "85b06184f6b3495db343f9828199a811",
            "d5577181077c4befb9df96398c1ce476",
            "e2506d2b6ff24fadafcccf3aee4f9ff8",
            "d4cdecf82cac4bd69ecd644136732a6b",
            "ffb6f76001684d9aa572ab1267035eb9",
            "7cf30f63b13744ba86910d5a2b65fe4a",
            "a818c7df4317410996a4f11f8264d41b",
            "b63b720b632844bbb9434b5acc42f647",
            "8b352d039355451da95fb6e7c72a40ab",
            "fee39189714a46a8a3f1e80282a26326",
            "6bf70287dc8c4948ad30628829f88cd2",
            "db1eb161e7e44db6aa9b30eacaf29855",
            "b0eda0507b7341f8b2e35bd642618439",
            "ef56bc55b1de48478f6764e660845c22",
            "0a0ed48c17eb4fdeabcb031001c54b2c",
            "3f5cdc90bf70437d8f6c7b90ed157d59",
            "34d7d4a7aa2e4ad3a6c79ae438770a99",
            "2867404b06b34fc99cfc7280503a5178",
            "3066745ca4ff4a22b7b02a1740701dc4"
          ]
        },
        "id": "oL8nzUj8tBPI",
        "outputId": "1674ee7f-7211-44f7-d083-15bd70a0568c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- GPT-2 Fine-tuning Setup ---\n",
            "Using device: cpu\n",
            "Loading tokenizer and model for 'gpt2'...\n",
            "Set tokenizer pad_token to eos_token\n",
            "Total trainable parameters (GPT-2): 124,439,808\n",
            "Tokenizing 5000 reviews for GPT-2 fine-tuning...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3dad7881bc641feac8e765a88db8d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1051 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b352d039355451da95fb6e7c72a40ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created LM dataset with 11144 blocks of size 128.\n",
            "Setting up Hugging Face Trainer...\n",
            "\n",
            "Starting GPT-2 fine-tuning for 1 epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='607' max='1393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 607/1393 2:52:55 < 3:44:38, 0.06 it/s, Epoch 0.44/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.995500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.975200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.940600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.943200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.959400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.965900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.963900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1393' max='1393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1393/1393 6:38:26, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>4.036900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.995500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.975200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>3.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>4.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>3.940600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>3.943200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>3.959400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>3.965900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>3.963900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>3.918300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>3.940200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>3.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>3.945900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>3.929500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>3.893700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>3.905700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>3.911900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>3.947500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>3.874900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>3.901400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>3.917100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>3.903600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>3.881900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>3.910100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished GPT-2 fine-tuning.\n",
            "Total Fine-tuning Time: 23933.07 seconds\n",
            "Saving final fine-tuned model to ./gpt2-finetuned-imdb-final...\n",
            "Model saved.\n",
            "\n",
            "Calculating Fine-tuned GPT-2 Perplexity...\n",
            "Fine-tuned GPT-2 Perplexity on correct sentences: 13.28\n",
            "Fine-tuned GPT-2 Perplexity on incorrect sentences: 308.82\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "import time\n",
        "import math\n",
        "\n",
        "# --- GPT-2 Fine-tuning Implementation ---\n",
        "print(\"\\n--- GPT-2 Fine-tuning Setup ---\")\n",
        "MODEL_NAME = \"gpt2\"  # Smallest GPT-2 model (117M parameters)\n",
        "GPT2_BATCH_SIZE = 8  # Smaller batch size for CPU fine-tuning\n",
        "GPT2_LEARNING_RATE = 5e-5\n",
        "GPT2_EPOCHS = 1  # Fine-tuning usually requires fewer epochs\n",
        "BLOCK_SIZE = 128  # Sequence length for GPT-2 training chunks\n",
        "GPT2_TRAIN_SIZE = 5000  # Use a smaller subset for faster fine-tuning demonstration\n",
        "\n",
        "# Set device (CPU or GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# --- 1. Load Tokenizer and Model ---\n",
        "print(f\"Loading tokenizer and model for '{MODEL_NAME}'...\")\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
        "# Set pad token if it doesn't exist (GPT-2 usually doesn't have one by default)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(\"Set tokenizer pad_token to eos_token\")\n",
        "\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "# Ensure the model's pad_token_id is configured, critical for DataCollator\n",
        "gpt2_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "gpt2_model.to(device)  # Move model to device\n",
        "\n",
        "# Print model's total parameters\n",
        "total_params_gpt2 = sum(p.numel() for p in gpt2_model.parameters() if p.requires_grad)\n",
        "print(f'Total trainable parameters (GPT-2): {total_params_gpt2:,}')\n",
        "\n",
        "# --- 2. Tokenize Dataset ---\n",
        "print(f\"Tokenizing {GPT2_TRAIN_SIZE} reviews for GPT-2 fine-tuning...\")\n",
        "# Use a subset of the dataframe for faster processing\n",
        "df_subset = df.iloc[:GPT2_TRAIN_SIZE].copy()\n",
        "\n",
        "# Convert pandas DataFrame to Hugging Face Dataset object\n",
        "hf_dataset = Dataset.from_pandas(df_subset)\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(examples):\n",
        "    # Tokenize texts\n",
        "    return tokenizer(examples[\"review\"], truncation=False)\n",
        "\n",
        "# Apply tokenization (batched=True for speed)\n",
        "tokenized_datasets = hf_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=[\"review\", \"sentiment\"]\n",
        ")\n",
        "\n",
        "# Define function to group texts into blocks\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # Drop the small remainder\n",
        "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
        "    # Split by chunks of block_size\n",
        "    result = {\n",
        "        k: [t[i:i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    # For language modeling, the labels are the inputs shifted by one\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "# Apply grouping (batched=True for efficiency)\n",
        "lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n",
        "print(f\"Created LM dataset with {len(lm_datasets)} blocks of size {BLOCK_SIZE}.\")\n",
        "\n",
        "# --- 3. Setup Hugging Face Trainer ---\n",
        "print(\"Setting up Hugging Face Trainer...\")\n",
        "\n",
        "# Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned-imdb\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=GPT2_EPOCHS,\n",
        "    per_device_train_batch_size=GPT2_BATCH_SIZE,\n",
        "    learning_rate=GPT2_LEARNING_RATE,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Data Collator for Language Modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Causal language modeling for GPT-2\n",
        ")\n",
        "\n",
        "# Instantiate Trainer\n",
        "trainer = Trainer(\n",
        "    model=gpt2_model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# --- 4. Fine-tuning ---\n",
        "print(f\"\\nStarting GPT-2 fine-tuning for {GPT2_EPOCHS} epochs...\")\n",
        "gpt2_tuning_start_time = time.time()\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "gpt2_tuning_end_time = time.time()\n",
        "gpt2_total_tuning_time = gpt2_tuning_end_time - gpt2_tuning_start_time\n",
        "print(f\"Finished GPT-2 fine-tuning.\")\n",
        "print(f\"Total Fine-tuning Time: {gpt2_total_tuning_time:.2f} seconds\")\n",
        "\n",
        "# --- 5. Save the Fine-tuned Model ---\n",
        "final_model_path = \"./gpt2-finetuned-imdb-final\"\n",
        "print(f\"Saving final fine-tuned model to {final_model_path}...\")\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(\"Model saved.\")\n",
        "\n",
        "# --- 6. GPT-2 Perplexity Calculation ---\n",
        "def calculate_gpt2_perplexity(model, tokenizer, sentences_text, device, block_size=1024):\n",
        "    \"\"\"Calculate perplexity for sentences using the fine-tuned GPT-2 model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in sentences_text:\n",
        "            encodings = tokenizer(text, return_tensors='pt', add_special_tokens=True)\n",
        "            input_ids = encodings.input_ids.to(device)\n",
        "            target_ids = input_ids.clone()\n",
        "\n",
        "            seq_len = input_ids.size(1)\n",
        "\n",
        "            if seq_len < 2:\n",
        "                print(f\"Skipping very short sentence: {text}\")\n",
        "                continue\n",
        "\n",
        "            outputs = model(input_ids, labels=target_ids)\n",
        "            loss = outputs.loss\n",
        "            num_tokens = seq_len - 1\n",
        "            if num_tokens > 0:\n",
        "                 total_loss += loss.item() * num_tokens\n",
        "                 total_tokens += num_tokens\n",
        "\n",
        "    if total_tokens == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    avg_ce_loss = total_loss / total_tokens\n",
        "    perplexity = math.exp(avg_ce_loss)\n",
        "    return perplexity\n",
        "\n",
        "# Example sentences for perplexity calculation\n",
        "correct_sentences_text = [\n",
        "    \"The movie was fantastic with great acting and an engaging plot.\",\n",
        "    \"I thoroughly enjoyed this film and would recommend it to anyone.\"\n",
        "]\n",
        "incorrect_sentences_text = [\n",
        "    \"Movie the was fantastic great acting and an plot engaging.\",\n",
        "    \"Enjoyed thoroughly I this film and would recommend it anyone to.\"\n",
        "]\n",
        "\n",
        "# Calculate and Report Perplexity\n",
        "print(\"\\nCalculating Fine-tuned GPT-2 Perplexity...\")\n",
        "\n",
        "gpt2_perplexity_correct = calculate_gpt2_perplexity(\n",
        "    gpt2_model, tokenizer, correct_sentences_text, device\n",
        ")\n",
        "gpt2_perplexity_incorrect = calculate_gpt2_perplexity(\n",
        "    gpt2_model, tokenizer, incorrect_sentences_text, device\n",
        ")\n",
        "\n",
        "print(f\"Fine-tuned GPT-2 Perplexity on correct sentences: {gpt2_perplexity_correct:.2f}\")\n",
        "print(f\"Fine-tuned GPT-2 Perplexity on incorrect sentences: {gpt2_perplexity_incorrect:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN0GBUBrtBPJ"
      },
      "source": [
        "# 6. Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9ADwlNutBPJ"
      },
      "source": [
        "We will now use each of the three models to generate text continuations based on a set of prompts.\n",
        "\n",
        "**Sampling Strategies:**\n",
        "*   **Greedy Search:** At each step, simply choose the word with the highest probability according to the model. This is deterministic but can lead to repetitive or dull text.\n",
        "*   **Sampling (Top-k / Top-p):** Instead of always picking the best word, introduce randomness.\n",
        "    *   **Top-k Sampling:** Consider only the `k` most likely next words and redistribute the probability mass among them. Then, sample from this reduced set. (e.g., k=50)\n",
        "    *   **Top-p (Nucleus) Sampling:** Consider the smallest set of most likely words whose cumulative probability mass exceeds a threshold `p`. Then, sample from this set. (e.g., p=0.9)\n",
        "\n",
        "Sampling methods often produce more diverse and interesting text compared to greedy search. For the neural models (LSTM, GPT-2), we will use the built-in capabilities or implement simple sampling. For the n-gram model, we will implement greedy generation for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM2Pwg5StBPJ",
        "outputId": "3d4edad5-6784-44ca-b131-75b4e8e0bfc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Generating with N-gram Model (Greedy) ---\n",
            "Prompt: 'I think' -> N-gram: 'I think 'm the the one most the part film the is best the of film the is best the of film the is best the of film the is best the'\n",
            "Prompt: 'She goes to' -> N-gram: 'She goes to the be best described the the one most the part film the is best the of film the is best the of film the is best the of film the'\n",
            "Prompt: 'The movie was' -> N-gram: 'The movie was the in movie the is best the of film the is best the of film the is best the of film the is best the of film the is best'\n",
            "Prompt: 'It felt like' -> N-gram: 'It felt like the . film the is best the of film the is best the of film the is best the of film the is best the of film the is best'\n",
            "Prompt: 'Blue dog' -> N-gram: 'Blue dog the , film and . the the one most the part film the is best the of film the is best the of film the is best the of film'\n",
            "\n",
            "--- Generating with LSTM Model (Greedy) ---\n",
            "Prompt: 'I think' -> LSTM: 'I think it is a'\n",
            "Prompt: 'She goes to' -> LSTM: 'She goes to the movie . the movie is a'\n",
            "Prompt: 'The movie was' -> LSTM: 'The movie was a'\n",
            "Prompt: 'It felt like' -> LSTM: 'It felt like the movie . i was a'\n",
            "Prompt: 'Blue dog' -> LSTM: 'Blue dog is a'\n",
            "\n",
            "--- Generating with Fine-tuned GPT-2 Model (Sampling) ---\n",
            "Loading fine-tuned model from ./gpt2-finetuned-imdb-final...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned model loaded successfully.\n",
            "Prompt: 'I think' -> GPT-2: 'I think you have your answer. We had one scene in this movie. It was the last scene. The movie is very cheesy and pretentious. We don'\n",
            "Prompt: 'She goes to' -> GPT-2: 'She goes to bed after a while, he is sleeping and wakes up to a note from his brother saying he was found dead and his father. The young couple has'\n",
            "Prompt: 'The movie was' -> GPT-2: 'The movie was terrible and almost as bad as \"Manhunt\" or \"A Day in the Life of the Lambs\" and the movie's main characters are so'\n",
            "Prompt: 'It felt like' -> GPT-2: 'It felt like a remake of the popular movie \"The Big Short\" with the idea of the director going from a young boy to a serious and committed man. But'\n",
            "Prompt: 'Blue dog' -> GPT-2: 'Blue dog's name and its dog's name are mentioned throughout the entire movie, but they're never really mentioned. It's very well-written, though I'\n",
            "\n",
            "--- Generation Results Summary ---\n",
            "|               | N-gram (Greedy)                                                                                                                              | LSTM (Greedy)                          | GPT-2 (Sampled)                                                                                                                                     |\n",
            "|:--------------|:---------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| I think       | I think 'm the the one most the part film the is best the of film the is best the of film the is best the of film the is best the            | I think it is a                        | I think you have your answer. We had one scene in this movie. It was the last scene. The movie is very cheesy and pretentious. We don               |\n",
            "| She goes to   | She goes to the be best described the the one most the part film the is best the of film the is best the of film the is best the of film the | She goes to the movie . the movie is a | She goes to bed after a while, he is sleeping and wakes up to a note from his brother saying he was found dead and his father. The young couple has |\n",
            "| The movie was | The movie was the in movie the is best the of film the is best the of film the is best the of film the is best the of film the is best       | The movie was a                        | The movie was terrible and almost as bad as \"Manhunt\" or \"A Day in the Life of the Lambs\" and the movie's main characters are so                    |\n",
            "| It felt like  | It felt like the . film the is best the of film the is best the of film the is best the of film the is best the of film the is best          | It felt like the movie . i was a       | It felt like a remake of the popular movie \"The Big Short\" with the idea of the director going from a young boy to a serious and committed man. But |\n",
            "| Blue dog      | Blue dog the , film and . the the one most the part film the is best the of film the is best the of film the is best the of film             | Blue dog is a                          | Blue dog's name and its dog's name are mentioned throughout the entire movie, but they're never really mentioned. It's very well-written, though I  |\n"
          ]
        }
      ],
      "source": [
        "# --- Text Generation Comparison ---\n",
        "prompts = [\n",
        "    \"I think\",\n",
        "    \"She goes to\",\n",
        "    \"The movie was\",\n",
        "    \"It felt like\",\n",
        "    \"Blue dog\"\n",
        "]\n",
        "MAX_GEN_LEN = 30 # Max number of tokens to generate after the prompt\n",
        "\n",
        "generation_results = defaultdict(list)\n",
        "\n",
        "# --- 1. N-gram Generation (Greedy) ---\n",
        "print(\"\\n--- Generating with N-gram Model (Greedy) ---\")\n",
        "def generate_ngram_greedy(prompt, n, max_len, prob_func, trigram_counts, bigram_counts, vocab, k=1):\n",
        "    tokens = preprocess_ngram(prompt, n)[:-1] # Preprocess but remove the final </s>\n",
        "    generated_tokens = []\n",
        "    vocab_list = list(vocab) # Convert set to list for iteration\n",
        "    vocab_size_gen = len(vocab_list)\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        if len(tokens) < n - 1:\n",
        "            # Should not happen with preprocessing, but handle defensively\n",
        "            break\n",
        "        # Get context (last n-1 tokens)\n",
        "        w1 = tokens[-(n-2)]\n",
        "        w2 = tokens[-(n-1)]\n",
        "\n",
        "        best_prob = -1\n",
        "        next_word = None\n",
        "\n",
        "        # Find the word w3 with the highest P(w3 | w1, w2)\n",
        "        for w3 in vocab_list:\n",
        "            # Skip start symbol as a potential next word\n",
        "            if w3 == '<s>': continue\n",
        "\n",
        "            prob = prob_func(w1, w2, w3, trigram_counts, bigram_counts, vocab_size_gen, k)\n",
        "            if prob > best_prob:\n",
        "                best_prob = prob\n",
        "                next_word = w3\n",
        "\n",
        "        if next_word is None or next_word == '</s>':\n",
        "            break # Stop if no word found or end symbol is generated\n",
        "\n",
        "        generated_tokens.append(next_word)\n",
        "        tokens.append(next_word)\n",
        "\n",
        "    return prompt + \" \" + \" \".join(generated_tokens)\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated_text = generate_ngram_greedy(prompt, n=3, max_len=MAX_GEN_LEN,\n",
        "                                           prob_func=get_add1_trigram_prob,\n",
        "                                           trigram_counts=trigram_counts,\n",
        "                                           bigram_counts=bigram_counts,\n",
        "                                           vocab=vocab, k=1)\n",
        "    generation_results[\"N-gram (Greedy)\"].append(generated_text)\n",
        "    print(f\"Prompt: '{prompt}' -> N-gram: '{generated_text}'\")\n",
        "\n",
        "# --- 2. LSTM Generation (Greedy / Sampling) ---\n",
        "print(\"\\n--- Generating with LSTM Model (Greedy) ---\")\n",
        "def generate_lstm(model, prompt, max_len, word_to_idx, idx_to_word, unk_idx, pad_idx, device, temperature=1.0, top_k=0):\n",
        "    model.eval()\n",
        "    tokens = word_tokenize(prompt.lower())\n",
        "    indexed = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
        "    generated_indices = []\n",
        "\n",
        "    input_tensor = torch.tensor([indexed], dtype=torch.long).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            # Get predictions (logits) for the last token\n",
        "            output = model(input_tensor)\n",
        "            # output shape: [1, current_seq_len, vocab_size]\n",
        "            last_token_logits = output[:, -1, :] # Shape: [1, vocab_size]\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            last_token_logits = last_token_logits / temperature\n",
        "\n",
        "            # Apply top-k filtering if k > 0\n",
        "            if top_k > 0:\n",
        "                indices_to_remove = last_token_logits < torch.topk(last_token_logits, top_k)[0][..., -1, None]\n",
        "                last_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "            # Get probabilities using softmax\n",
        "            probabilities = torch.softmax(last_token_logits, dim=-1)\n",
        "\n",
        "            # Sample next token index\n",
        "            # Use multinomial for sampling, or argmax for greedy\n",
        "            # next_token_idx = torch.multinomial(probabilities, num_samples=1).item()\n",
        "            next_token_idx = torch.argmax(probabilities, dim=-1).item() # Greedy\n",
        "\n",
        "            # Stop if PAD or UNK generated (or EOS if defined)\n",
        "            if next_token_idx == pad_idx or next_token_idx == unk_idx:\n",
        "                 break\n",
        "\n",
        "            generated_indices.append(next_token_idx)\n",
        "\n",
        "            # Append the predicted token index to the input sequence for the next step\n",
        "            next_token_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
        "            input_tensor = torch.cat([input_tensor, next_token_tensor], dim=1)\n",
        "\n",
        "    generated_words = [idx_to_word.get(idx, UNK_TOKEN) for idx in generated_indices]\n",
        "    return prompt + \" \" + \" \".join(generated_words)\n",
        "\n",
        "for prompt in prompts:\n",
        "    generated_text = generate_lstm(lstm_model, prompt, max_len=MAX_GEN_LEN,\n",
        "                                   word_to_idx=word_to_idx, idx_to_word=idx_to_word,\n",
        "                                   unk_idx=UNK_IDX, pad_idx=PAD_IDX, device=device,\n",
        "                                   temperature=1.0, top_k=0) # Greedy (top_k=0)\n",
        "    generation_results[\"LSTM (Greedy)\"].append(generated_text)\n",
        "    print(f\"Prompt: '{prompt}' -> LSTM: '{generated_text}'\")\n",
        "\n",
        "# --- 3. GPT-2 Generation (Using model.generate) ---\n",
        "print(\"\\n--- Generating with Fine-tuned GPT-2 Model (Sampling) ---\")\n",
        "# Load the saved fine-tuned model for generation\n",
        "print(f\"Loading fine-tuned model from {final_model_path}...\")\n",
        "try:\n",
        "    gpt2_model_loaded = GPT2LMHeadModel.from_pretrained(final_model_path)\n",
        "    tokenizer_loaded = GPT2TokenizerFast.from_pretrained(final_model_path)\n",
        "    gpt2_model_loaded.to(device)\n",
        "    print(\"Fine-tuned model loaded successfully.\")\n",
        "\n",
        "    for prompt in prompts:\n",
        "        # Encode the prompt\n",
        "        input_ids = tokenizer_loaded.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "        # Generate text using sampling (top-k, top-p)\n",
        "        # Ensure pad_token_id is set for generation\n",
        "        pad_token_id = tokenizer_loaded.pad_token_id if tokenizer_loaded.pad_token_id is not None else tokenizer_loaded.eos_token_id\n",
        "\n",
        "        output_sequences = gpt2_model_loaded.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_length=len(input_ids[0]) + MAX_GEN_LEN, # Generate MAX_GEN_LEN new tokens\n",
        "            temperature=1.0, # Controls randomness. Higher values = more random.\n",
        "            top_k=50,        # Considers only the top 50 words\n",
        "            top_p=0.9,       # Nucleus sampling: cumulative probability threshold\n",
        "            do_sample=True,  # Enable sampling\n",
        "            num_return_sequences=1, # Generate one sequence\n",
        "            pad_token_id=pad_token_id # Set pad token id\n",
        "        )\n",
        "\n",
        "        # Decode the generated sequence\n",
        "        generated_text = tokenizer_loaded.decode(output_sequences[0], skip_special_tokens=True)\n",
        "        generation_results[\"GPT-2 (Sampled)\"].append(generated_text)\n",
        "        print(f\"Prompt: '{prompt}' -> GPT-2: '{generated_text}'\")\n",
        "\n",
        "except OSError as e:\n",
        "     print(f\"Error loading fine-tuned model from {final_model_path}: {e}\")\n",
        "     print(\"Skipping GPT-2 generation. Ensure the model was saved correctly.\")\n",
        "     for prompt in prompts:\n",
        "         generation_results[\"GPT-2 (Sampled)\"].append(prompt + \" [Error: Model not loaded]\")\n",
        "\n",
        "# --- 4. Display Results in a Table ---\n",
        "print(\"\\n--- Generation Results Summary ---\")\n",
        "generation_df = pd.DataFrame(generation_results)\n",
        "generation_df.index = prompts # Use prompts as index for clarity\n",
        "print(generation_df.to_markdown()) # Print as markdown table for better readability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teEOYAFTtBPJ"
      },
      "source": [
        "# 7. Comparison & Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIojJvrCtBPJ"
      },
      "source": [
        "**Summary Table**\n",
        "\n",
        "| Feature                  | N-gram (Trigram, Add-1) | LSTM (from Scratch)        | GPT-2 (Fine-tuned)        |\n",
        "| :----------------------- | :---------------------- | :------------------------- | :------------------------ |\n",
        "| Approx. Training Time    | ~ Seconds/Minutes (Counts) | ~ Hours (on CPU, 5 Epochs) | ~ Hours (on CPU, 1 Epoch) |\n",
        "| # Trainable Parameters   | ~ 0 (Counts based)      | ~ 7.5 Million (Example)    | ~ 117 Million             |\n",
        "| Avg. Perplexity (Correct)| [Fill in from Sec 3]    | [Fill in from Sec 4]       | [Fill in from Sec 5]      |\n",
        "| Avg. Perplexity (Incorrect)| [Fill in from Sec 3]    | [Fill in from Sec 4]       | [Fill in from Sec 5]      |\n",
        "| Sample Quality Notes     | Often repetitive, grammatically simple/incorrect. Struggles with long context. | Can capture local structure, potentially repetitive. Quality depends heavily on training. | Generally most coherent, fluent, contextually relevant due to pre-training. |\n",
        "\n",
        "*Note: Training times are rough estimates for the specified subset sizes and epochs on a typical modern CPU. Actual times will vary.*\n",
        "*Note: Parameter count for LSTM depends on the exact configuration (vocab size, embedding/hidden dims).*\n",
        "*Note: Perplexity values need to be filled in after running the notebook.*\n",
        "\n",
        "---\n",
        "\n",
        "**1. Which model best balances quality and resources?**\n",
        "\n",
        "*   **N-gram:** Very low resource usage (fast counting, minimal memory), but lowest quality. Poor generalization and generation.\n",
        "*   **LSTM from Scratch:** Moderate resource usage (significant CPU time for training, moderate parameters/memory). Quality can be decent but requires careful tuning and substantial training. It strikes a balance if pre-trained models are not an option or if full control over the architecture is needed.\n",
        "*   **Fine-tuned GPT-2:** High resource usage *during fine-tuning* (significant CPU time, high memory), but leverages massive pre-training. Offers the highest quality by far, especially for generation fluency and coherence. If the fine-tuning time (which is often less than training a large LSTM from scratch) is acceptable, **fine-tuned GPT-2 likely offers the best balance between *achievable quality* and *fine-tuning effort***, assuming the pre-trained model itself is available.\n",
        "\n",
        "The \"best\" balance depends on the specific constraints. If training time is severely limited, n-grams are fastest. If state-of-the-art quality is paramount, fine-tuning is the way to go. The LSTM occupies a middle ground.\n",
        "\n",
        "**2. How could each model be improved?**\n",
        "\n",
        "*   **N-gram:**\n",
        "    *   *Better Smoothing:* Use more advanced techniques like Kneser-Ney smoothing instead of Add-1.\n",
        "    *   *Backoff/Interpolation:* Combine probabilities from trigrams, bigrams, and unigrams instead of relying solely on trigrams.\n",
        "    *   *Larger Corpus:* Train on more data (though returns diminish).\n",
        "*   **LSTM from Scratch:**\n",
        "    *   *More Data & Training:* Train on the full dataset for more epochs (requires more time/resources).\n",
        "    *   *Larger Model:* Increase embedding size, hidden size, or number of layers.\n",
        "    *   *Hyperparameter Tuning:* Optimize learning rate, batch size, dropout, weight decay.\n",
        "    *   *Better Tokenization:* Use subword tokenization (like BPE) instead of word tokenization to handle rare words better.\n",
        "    *   *Architecture:* Add attention mechanisms or switch to a Transformer architecture (though this becomes much more complex).\n",
        "*   **Fine-tuned GPT-2:**\n",
        "    *   *More Fine-tuning Data/Epochs:* Fine-tune on more data or for slightly longer (careful not to overfit).\n",
        "    *   *Larger Pre-trained Model:* Use GPT-2 Medium (345M) or Large (774M) if resources allow (requires significantly more RAM/time).\n",
        "    *   *Domain Adaptation:* Continue pre-training on a large corpus of movie-related text before fine-tuning on reviews.\n",
        "    *   *Parameter-Efficient Fine-tuning (PEFT):* Techniques like LoRA can reduce the computational cost of fine-tuning very large models.\n",
        "\n",
        "**3. What difficulties were encountered?**\n",
        "\n",
        "*   **CPU Training Time:** Training the LSTM from scratch and fine-tuning GPT-2 on a CPU is very time-consuming. Even with subsets of the data, training can take hours. This highlights the significant advantage of GPUs for deep learning.\n",
        "*   **Memory Usage:** Loading and fine-tuning large models like GPT-2, even the small version, can require substantial RAM, potentially limiting batch sizes or feasibility on some systems.\n",
        "*   **N-gram Implementation:** Correctly implementing smoothing and perplexity for n-grams, especially handling edge cases and context counts accurately, can be tricky.\n",
        "*   **Hyperparameter Sensitivity:** Neural models (LSTM, GPT-2 fine-tuning) are sensitive to hyperparameters like learning rate and batch size. Finding good settings without extensive experimentation can be challenging.\n",
        "*   **Vocabulary Management:** Handling large vocabularies, out-of-vocabulary words (UNK tokens), and padding consistently across preprocessing, model definition, and loss calculation requires care.\n",
        "*   **Dependency Management:** Ensuring all libraries (PyTorch, Transformers, NLTK, Datasets, Accelerate) are installed and compatible can sometimes be an issue.\n",
        "*   **Perplexity Calculation Nuances:** Ensuring the perplexity calculation is consistent and correct for each model type (handling start/end tokens for n-grams, padding/masking for neural models, log probabilities for stability) requires careful implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOZXonrtBPK"
      },
      "source": [
        "# Appendix: Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nywNR0wwtBPK"
      },
      "source": [
        "This notebook was tested with the following core package versions. You can recreate the environment using pip:\n",
        "\n",
        "```\n",
        "# Run `pip freeze` in your environment after installation\n",
        "# Example versions (actual versions might differ slightly based on install date):\n",
        "accelerate==0.24.1\n",
        "datasets==2.15.0\n",
        "nltk==3.8.1\n",
        "numpy==1.26.2\n",
        "pandas==2.1.1\n",
        "torch==2.1.0\n",
        "transformers==4.35.2\n",
        "# Other dependencies installed automatically might include:\n",
        "# huggingface-hub, packaging, pyarrow, regex, requests, safetensors, tokenizers, tqdm, etc.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7KAif9S0YsV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a0ed48c17eb4fdeabcb031001c54b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2867404b06b34fc99cfc7280503a5178": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3066745ca4ff4a22b7b02a1740701dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34d7d4a7aa2e4ad3a6c79ae438770a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f5cdc90bf70437d8f6c7b90ed157d59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bf70287dc8c4948ad30628829f88cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5cdc90bf70437d8f6c7b90ed157d59",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34d7d4a7aa2e4ad3a6c79ae438770a99",
            "value": 5000
          }
        },
        "7cf30f63b13744ba86910d5a2b65fe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81f98b26c4204b1594ff5f6166e5cfce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb6f76001684d9aa572ab1267035eb9",
            "max": 5000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cf30f63b13744ba86910d5a2b65fe4a",
            "value": 5000
          }
        },
        "85b06184f6b3495db343f9828199a811": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a818c7df4317410996a4f11f8264d41b",
            "placeholder": "",
            "style": "IPY_MODEL_b63b720b632844bbb9434b5acc42f647",
            "value": "5000/5000[00:10&lt;00:00,482.76examples/s]"
          }
        },
        "8b352d039355451da95fb6e7c72a40ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fee39189714a46a8a3f1e80282a26326",
              "IPY_MODEL_6bf70287dc8c4948ad30628829f88cd2",
              "IPY_MODEL_db1eb161e7e44db6aa9b30eacaf29855"
            ],
            "layout": "IPY_MODEL_b0eda0507b7341f8b2e35bd642618439"
          }
        },
        "a818c7df4317410996a4f11f8264d41b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0eda0507b7341f8b2e35bd642618439": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b63b720b632844bbb9434b5acc42f647": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d3dad7881bc641feac8e765a88db8d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f984c087688640bab1e60b748c05e9fb",
              "IPY_MODEL_81f98b26c4204b1594ff5f6166e5cfce",
              "IPY_MODEL_85b06184f6b3495db343f9828199a811"
            ],
            "layout": "IPY_MODEL_d5577181077c4befb9df96398c1ce476"
          }
        },
        "d4cdecf82cac4bd69ecd644136732a6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5577181077c4befb9df96398c1ce476": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db1eb161e7e44db6aa9b30eacaf29855": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2867404b06b34fc99cfc7280503a5178",
            "placeholder": "",
            "style": "IPY_MODEL_3066745ca4ff4a22b7b02a1740701dc4",
            "value": "5000/5000[00:17&lt;00:00,296.92examples/s]"
          }
        },
        "e2506d2b6ff24fadafcccf3aee4f9ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef56bc55b1de48478f6764e660845c22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f984c087688640bab1e60b748c05e9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2506d2b6ff24fadafcccf3aee4f9ff8",
            "placeholder": "",
            "style": "IPY_MODEL_d4cdecf82cac4bd69ecd644136732a6b",
            "value": "Map:100%"
          }
        },
        "fee39189714a46a8a3f1e80282a26326": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef56bc55b1de48478f6764e660845c22",
            "placeholder": "",
            "style": "IPY_MODEL_0a0ed48c17eb4fdeabcb031001c54b2c",
            "value": "Map:100%"
          }
        },
        "ffb6f76001684d9aa572ab1267035eb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
